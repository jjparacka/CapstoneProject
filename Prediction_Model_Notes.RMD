---
title: 'Capstone Project : Word Prediction Model'
author: "Jacob Paracka"
date: "March 16, 2018"
output: html_document
---

##Introduction
- The objective of the project is to build a word prediction model that will aid typing by predicting the most probable next words based on the previous words/phrases entered. This is the final delivery for Coursera's Data Science Specialization: Swiftkey Capstone Project
- The prediction model is developed based on a data set consisting of blogs, news articles and tweets
- A web based application is developed (Shiny io), that reads an input from the user that uses the model to predict the three most probable next words. 
- The application is designed to enable users to easily select the predicted words and complete the sentence 
- Special emphasis is made to tune the model for optimum memory utilization and accuracy. Some trade-offs are mode so that the application is light and fast.

##Model building - Highlights

The model if built from a samples of 200,000 blogs, news and tweets each. The sample size was selected to get as much data which could be processed in a reasonable time period.

Step 1 Data Cleaning and preprocessing 

- remove all punctuations
- conver to all lower case 
- remove special characters and empty space 
- remove profanity 

Step2 Building NGrams

- use the quanteda package to build NGrams with lengths 1 to 6 
- get the count of each NGram in the data and arrange them by frequency 

Step3: Algorithm 

- The last N-1 words from the input is matched with the initial N-1 words in the NGram. If a match is found , the last word in the NGram is the prediction. The frequency of that NGram is the weightage of the prediction
- The matching is done starting from the 6Grams to the 2Grams. Matches from longer NGrams are given higher weightage
- Next word prediction is triggered when the user inputs a space, indicating the end of the previous word. While a word is being typed, the application looks up the uni-gram for suggestions of the current word.

#Model Performance Tuining

Using NGrams of lenghts 6 and 5 proved to use high memory andn was not very responsive. Also noticed that a large part of these Ngrams were single occurances. Exploratory analysis showed that theses are a result of errors in spelling or incorrect usage of words. 

Possible tuining approaches 

- Change number of NGrams used in prediction
- Optimize single occurance NGams (the long tail) 

A preformance measurement for the prediction was done with various combination of NGrams. The results are below. 

```{r modelperformance, echo=FALSE}
require(ggplot2)
modelid <- c("model1","model2","model3","model4","model5","model6","model7","model8","model9")
Accuracy <- c(16.58, 16.62, 16.82, 16.68, 15.79, 17.38, 17.38, 17.06, 15.53)
Memory <- c(2399.84,1621.53,953.26, 432.72, 121.94,299.24, 215.97,153.64,91.62    )
d<- data.frame(modelid, Accuracy, Memory, stringsAsFactors = FALSE)
d$Modeldesc <- 
  c(
    "Model 1: use full set of 6-grams to 1-grams for prediction" ,
    "Model 2: use full set of 5-grams to 1-grams for prediction" ,
    "Model 3: use full set of 4-grams to 1-grams for prediction" ,
    "Model 4: use full set of 3-grams to 1-grams for prediction" ,
    "Model 5: use full set of 2-grams to 1-grams for prediction" ,
    "Model 6: using top 20 of 5-grams to 1-grams for prediction" ,
    "Model 7: using top 20 of 4-grams to 1-grams for prediction" ,
    "Model 8: using top 20 of 3-grams to 1-grams for prediction" ,
    "Model 9: using top 20 of 2-grams to 1-grams for prediction" 
  )

d<- d[order(d$Memory),]

ggplot(d, aes(Modeldesc)) + 
  geom_line(aes(y = Memory, colour = "Memory", group=1)) + 
  geom_line(aes(y = Accuracy*100, colour = "Accuracy" , group=1))+ 
  scale_y_continuous(sec.axis = sec_axis(~ ./100, name = "Accuracy%")) + 
  geom_vline(xintercept = 7) + 
  theme(axis.text.x = element_text(angle = 70, hjust = 1))
```

In favor of performance the following changes were made to the model
- user only QuadGram, TriGram and BiGrams for prediction 
- For each N-1 group of words, use only the top 20 rows in the NGrams.
- Remove Single occurance NGrams
The prediction accuracy was not impacted by these changes. 

#Word Predition Application 
The application is designed to take the user input and provides the top 3 predictions. 
User can select the predicted words by clicking on the corresponding buttons. Thus reducing the number of key strokes needed to complete the sentence. 
Application also has a current word predictions capability based on a partial word input
A word cloud of the top 2- predictions is also displayed as suggestions beyond the top3 

# Conclusion

- Using a small data set and a simple algorithm a relatively good prediction model could be developed.
- The shiny framework provides and very easy way to build applications
- This project was definitely a great hands on experience in data science.
